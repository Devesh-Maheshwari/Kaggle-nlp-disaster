{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D,Convolution1D,Dropout,MaxPooling1D,Conv1D,Input,Flatten\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Exploring dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet= pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape (7613, 5)\n",
      "test data shape (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "print('training data shape', tweet.shape)\n",
    "print('test data shape', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_data(df):\n",
    "    \n",
    "    '''Input- df= pandas dataframes to be explored\n",
    "       Output- print shape, info and first 5 records of the dataframe \n",
    "    '''\n",
    "    \n",
    "    print(\"-\"*50)\n",
    "    print('Shape of the dataframe:',df.shape)\n",
    "    print(\"Number of records in train data set:\",df.shape[0])\n",
    "    print(\"Information of the dataset:\")\n",
    "    df.info()\n",
    "    print(\"-\"*50)\n",
    "    print(\"First 5 records of the dataset:\")\n",
    "    return df.head()\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Shape of the dataframe: (7613, 5)\n",
      "Number of records in train data set: 7613\n",
      "Information of the dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      "id          7613 non-null int64\n",
      "keyword     7552 non-null object\n",
      "location    5080 non-null object\n",
      "text        7613 non-null object\n",
      "target      7613 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n",
      "--------------------------------------------------\n",
      "First 5 records of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets use explore_data() function to explore train data\n",
    "explore_data(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Shape of the dataframe: (3263, 4)\n",
      "Number of records in train data set: 3263\n",
      "Information of the dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      "id          3263 non-null int64\n",
      "keyword     3237 non-null object\n",
      "location    2158 non-null object\n",
      "text        3263 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.0+ KB\n",
      "--------------------------------------------------\n",
      "First 5 records of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets use explore_data() function to explore test data\n",
    "explore_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target of 0 is 57 % of total\n",
      "Target of 1 is 43 % of total\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "feature='target'\n",
    "sns.countplot(feature, data=tweet)\n",
    "print('Target of 0 is {} % of total'.format(round(tweet[feature].value_counts()[0]/len(tweet[feature])*100)))\n",
    "print('Target of 1 is {} % of total'.format(round(tweet[feature].value_counts()[1]/len(tweet[feature])*100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "Before starting any NLP project, text data needs to be pre-processed to convert it into in a consistent format.Text will be cleaned, tokneized and converted into a matrix.\n",
    "\n",
    "Some of the basic text pre-processing techniques includes:\n",
    "1. **Make text all lower or uppercase**\n",
    "2. **Removing Noise** - Remove Punctuation and numerical Values\n",
    "3. **Tokenization**  - Process of converting the normal text strings into a list of tokens i.e. words.\n",
    "4. **Stopword Removal**-Some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely.\n",
    "5. **Stemming**-Stemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form — generally a written word form. Example if we were to stem the following words: “Stems”, “Stemming”, “Stemmed”, “and Stemtization”, the result would be a single word “stem”.\n",
    "6. **Lemmatization**-A slight variant of stemming is lemmatization. The major difference between these is, that, stemming can often create non-existent words, whereas lemmas are actual words. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma. Examples of Lemmatization are that “run” is a base form for words like “running” or “ran” or that the word “better” and “good” are in the same lemma so they are considered the same.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to clean the text\n",
    "def clean_text(text):\n",
    "\n",
    "    '''\n",
    "    Input- 'text' to be cleaned\n",
    "       \n",
    "       Output- Convert input 'text' to lowercase,remove square brackets,links,punctuation\n",
    "       and words containing numbers. Return clean text.\n",
    "    \n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df1=tweet.copy()\n",
    "test_df1=test.copy()\n",
    "tweet_df1['text'] = tweet_df1['text'].apply(lambda x: clean_text(x))\n",
    "test_df1['text'] = test_df1['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets look at cleaned data\n",
    "def text_after_preprocess(before_text,after_text):\n",
    "    \n",
    "    '''\n",
    "    Input- before_text=text column before cleanup\n",
    "              after_text= text column after cleanup\n",
    "       Output- print before and after text to compare how it looks after cleanup\n",
    "       \n",
    "    '''\n",
    "    print('-'*60)\n",
    "    print('Text before cleanup')\n",
    "    print('-'*60)\n",
    "    print(before_text.head(5))\n",
    "    print('-'*60)\n",
    "    print('Text after cleanup')\n",
    "    print('-'*60)\n",
    "    print(after_text.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Text before cleanup\n",
      "------------------------------------------------------------\n",
      "0    Our Deeds are the Reason of this #earthquake M...\n",
      "1               Forest fire near La Ronge Sask. Canada\n",
      "2    All residents asked to 'shelter in place' are ...\n",
      "3    13,000 people receive #wildfires evacuation or...\n",
      "4    Just got sent this photo from Ruby #Alaska as ...\n",
      "Name: text, dtype: object\n",
      "------------------------------------------------------------\n",
      "Text after cleanup\n",
      "------------------------------------------------------------\n",
      "0    our deeds are the reason of this earthquake ma...\n",
      "1                forest fire near la ronge sask canada\n",
      "2    all residents asked to shelter in place are be...\n",
      "3     people receive wildfires evacuation orders in...\n",
      "4    just got sent this photo from ruby alaska as s...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "text_after_preprocess(tweet.text,tweet_df1.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Text before cleanup\n",
      "------------------------------------------------------------\n",
      "0                   Just happened a terrible car crash\n",
      "1    Heard about #earthquake is different cities, s...\n",
      "2    there is a forest fire at spot pond, geese are...\n",
      "3             Apocalypse lighting. #Spokane #wildfires\n",
      "4        Typhoon Soudelor kills 28 in China and Taiwan\n",
      "Name: text, dtype: object\n",
      "------------------------------------------------------------\n",
      "Text after cleanup\n",
      "------------------------------------------------------------\n",
      "0                   just happened a terrible car crash\n",
      "1    heard about earthquake is different cities sta...\n",
      "2    there is a forest fire at spot pond geese are ...\n",
      "3                apocalypse lighting spokane wildfires\n",
      "4          typhoon soudelor kills  in china and taiwan\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "text_after_preprocess(test.text,test_df1.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Example Text:  Heard about #earthquake is different cities, stay safe everyone.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tokenization by whitespace:-  ['Heard', 'about', '#earthquake', 'is', 'different', 'cities,', 'stay', 'safe', 'everyone.']\n",
      "Tokenization by words using Treebank Word Tokenizer:-  ['Heard', 'about', '#', 'earthquake', 'is', 'different', 'cities', ',', 'stay', 'safe', 'everyone', '.']\n",
      "Tokenization by punctuation:-  ['Heard', 'about', '#', 'earthquake', 'is', 'different', 'cities', ',', 'stay', 'safe', 'everyone', '.']\n",
      "Tokenization by regular expression:-  ['Heard', 'about', 'earthquake', 'is', 'different', 'cities', 'stay', 'safe', 'everyone']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Example how tokenization of text works\n",
    "text = \"Heard about #earthquake is different cities, stay safe everyone.\"\n",
    "tokenizer1 = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokenizer2 = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer3 = nltk.tokenize.WordPunctTokenizer()\n",
    "tokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "print(\"-\"*100)\n",
    "print(\"Example Text: \",text)\n",
    "print(\"-\"*100)\n",
    "print(\"Tokenization by whitespace:- \",tokenizer1.tokenize(text))\n",
    "print(\"Tokenization by words using Treebank Word Tokenizer:- \",tokenizer2.tokenize(text))\n",
    "print(\"Tokenization by punctuation:- \",tokenizer3.tokenize(text))\n",
    "print(\"Tokenization by regular expression:- \",tokenizer4.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  our deeds are the reason of this earthquake ma...   \n",
       "1   4     NaN      NaN              forest fire near la ronge sask canada   \n",
       "2   5     NaN      NaN  all residents asked to shelter in place are be...   \n",
       "3   6     NaN      NaN   people receive wildfires evacuation orders in...   \n",
       "4   7     NaN      NaN  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#before tokenization\n",
    "tweet_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Tokenize the training and the test dataset copies with RegEx tokenizer\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "tweet_df1['text'] = tweet_df1['text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "test_df1['text'] = test_df1['text'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [our, deeds, are, the, reason, of, this, earth...\n",
       "1           [forest, fire, near, la, ronge, sask, canada]\n",
       "2       [all, residents, asked, to, shelter, in, place...\n",
       "3       [people, receive, wildfires, evacuation, order...\n",
       "4       [just, got, sent, this, photo, from, ruby, ala...\n",
       "5       [rockyfire, update, california, hwy, closed, i...\n",
       "6       [flood, disaster, heavy, rain, causes, flash, ...\n",
       "7       [im, on, top, of, the, hill, and, i, can, see,...\n",
       "8       [theres, an, emergency, evacuation, happening,...\n",
       "9       [im, afraid, that, the, tornado, is, coming, t...\n",
       "10      [three, people, died, from, the, heat, wave, s...\n",
       "11      [haha, south, tampa, is, getting, flooded, hah...\n",
       "12      [raining, flooding, florida, tampabay, tampa, ...\n",
       "13          [flood, in, bago, myanmar, we, arrived, bago]\n",
       "14      [damage, to, school, bus, on, in, multi, car, ...\n",
       "15                                       [whats, up, man]\n",
       "16                                      [i, love, fruits]\n",
       "17                                   [summer, is, lovely]\n",
       "18                                [my, car, is, so, fast]\n",
       "19                             [what, a, goooooooaaaaaal]\n",
       "20                                 [this, is, ridiculous]\n",
       "21                                     [london, is, cool]\n",
       "22                                         [love, skiing]\n",
       "23                              [what, a, wonderful, day]\n",
       "24                                             [looooool]\n",
       "25                      [no, wayi, cant, eat, that, shit]\n",
       "26                             [was, in, nyc, last, week]\n",
       "27                                 [love, my, girlfriend]\n",
       "28                                               [cooool]\n",
       "29                                 [do, you, like, pasta]\n",
       "                              ...                        \n",
       "7583    [pic, of, old, pkk, suicide, bomber, who, deto...\n",
       "7584    [these, boxes, are, ready, to, explode, explod...\n",
       "7585    [calgary, police, flood, road, closures, in, c...\n",
       "7586    [sismo, detectado, japìn, seismic, intensity, ...\n",
       "7587                                 [sirens, everywhere]\n",
       "7588    [breaking, isis, claims, responsibility, for, ...\n",
       "7589                                    [omg, earthquake]\n",
       "7590    [severe, weather, bulletin, no, for, typhoon, ...\n",
       "7591    [heat, wave, warning, aa, ayyo, dei, just, whe...\n",
       "7592    [an, is, group, suicide, bomber, detonated, an...\n",
       "7593    [i, just, heard, a, really, loud, bang, and, e...\n",
       "7594    [a, gas, thing, just, exploded, and, i, heard,...\n",
       "7595    [nws, flash, flood, warning, continued, for, s...\n",
       "7596    [rt, livingsafely, nws, issues, severe, thunde...\n",
       "7597    [aircraft, debris, found, on, la, reunion, is,...\n",
       "7598    [fatherofthree, lost, control, of, car, after,...\n",
       "7599    [earthquake, in, ssw, of, anza, california, ip...\n",
       "7600    [evacuation, order, lifted, for, town, of, roo...\n",
       "7601    [breaking, la, refugio, oil, spill, may, have,...\n",
       "7602    [a, siren, just, went, off, and, it, wasnt, th...\n",
       "7603    [officials, say, a, quarantine, is, in, place,...\n",
       "7604    [worldnews, fallen, powerlines, on, glink, tra...\n",
       "7605    [on, the, flip, side, im, at, walmart, and, th...\n",
       "7606    [suicide, bomber, kills, in, saudi, security, ...\n",
       "7607    [stormchase, violent, record, breaking, el, re...\n",
       "7608    [two, giant, cranes, holding, a, bridge, colla...\n",
       "7609    [ariaahrary, thetawniest, the, out, of, contro...\n",
       "7610                             [s, of, volcano, hawaii]\n",
       "7611    [police, investigating, after, an, ebike, coll...\n",
       "7612    [the, latest, more, homes, razed, by, northern...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check tokenized text\n",
    "tweet_df1['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a funtion to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input- text=text from which english stopwprds will be removed\n",
    "    Output- return text without english stopwords \n",
    "    \n",
    "    \"\"\"\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  [our, deeds, are, the, reason, of, this, earth...   \n",
       "1   4     NaN      NaN      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2   5     NaN      NaN  [all, residents, asked, to, shelter, in, place...   \n",
       "3   6     NaN      NaN  [people, receive, wildfires, evacuation, order...   \n",
       "4   7     NaN      NaN  [just, got, sent, this, photo, from, ruby, ala...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Before stopwords removal\n",
    "tweet_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df1['text'] = tweet_df1['text'].apply(lambda x : remove_stopwords(x))\n",
    "test_df1['text'] = test_df1['text'].apply(lambda x : remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  [deeds, reason, earthquake, may, allah, forgiv...   \n",
       "1   4     NaN      NaN      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2   5     NaN      NaN  [residents, asked, shelter, place, notified, o...   \n",
       "3   6     NaN      NaN  [people, receive, wildfires, evacuation, order...   \n",
       "4   7     NaN      NaN  [got, sent, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#after stopwords removal\n",
    "tweet_df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Stemming and lemmatization sometimes doesnt necessarily improve results as sometimes we dont want to trim words rather preserve their original form.Its usage from problem to problem and for this problem it wouldnt be good idea to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and Lemmatization examples\n",
    "text =  [ 'deduced', 'dogs', 'talking', 'studies']\n",
    "def Stemming_Lemmatizing(text):\n",
    "    # Lemmatizer\n",
    "    lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "    words=[lemmatizer.lemmatize(token) for token in text]\n",
    "    \n",
    "    # Stemmer\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    words=[stemmer.stem(token) for token in text]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/devesh/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['deduc', 'dog', 'talk', 'studi']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "Stemming_Lemmatizing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_df1['text'] = tweet_df1['text'].apply(lambda x : Stemming_Lemmatizing(x))\n",
    "#test_df1['text'] = test_df1['text'].apply(lambda x : Stemming_Lemmatizing(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the text list into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geeks for Geeks\n"
     ]
    }
   ],
   "source": [
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \" \" \n",
    "    \n",
    "    # return string   \n",
    "    return (str1.join(s)) \n",
    "        \n",
    "        \n",
    "# Driver code     \n",
    "s = ['Geeks', 'for', 'Geeks'] \n",
    "print(listToString(s))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_df1['text'] = tweet_df1['text'].apply(lambda x : listToString(x))\n",
    "#test_df1['text'] = test_df1['text'].apply(lambda x : listToString(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.5.4-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.5.4\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\n",
      "You should consider upgrading via the '/home/devesh/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'correct me please'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "        \n",
    "text = \"corect me plese\"\n",
    "correct_spellings(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [deeds, reason, earthquake, may, allah, forgiv...\n",
       "1           [forest, fire, near, la, ronge, sask, canada]\n",
       "2       [residents, asked, shelter, place, notified, o...\n",
       "3       [people, receive, wildfires, evacuation, order...\n",
       "4       [got, sent, photo, ruby, alaska, smoke, wildfi...\n",
       "5       [rockyfire, update, california, hwy, closed, d...\n",
       "6       [flood, disaster, heavy, rain, causes, flash, ...\n",
       "7                       [im, top, hill, see, fire, woods]\n",
       "8       [theres, emergency, evacuation, happening, bui...\n",
       "9                     [im, afraid, tornado, coming, area]\n",
       "10                 [three, people, died, heat, wave, far]\n",
       "11      [haha, south, tampa, getting, flooded, hah, wa...\n",
       "12      [raining, flooding, florida, tampabay, tampa, ...\n",
       "13                  [flood, bago, myanmar, arrived, bago]\n",
       "14      [damage, school, bus, multi, car, crash, break...\n",
       "15                                           [whats, man]\n",
       "16                                         [love, fruits]\n",
       "17                                       [summer, lovely]\n",
       "18                                            [car, fast]\n",
       "19                                      [goooooooaaaaaal]\n",
       "20                                           [ridiculous]\n",
       "21                                         [london, cool]\n",
       "22                                         [love, skiing]\n",
       "23                                       [wonderful, day]\n",
       "24                                             [looooool]\n",
       "25                                [wayi, cant, eat, shit]\n",
       "26                                      [nyc, last, week]\n",
       "27                                     [love, girlfriend]\n",
       "28                                               [cooool]\n",
       "29                                          [like, pasta]\n",
       "                              ...                        \n",
       "7583    [pic, old, pkk, suicide, bomber, detonated, bo...\n",
       "7584    [boxes, ready, explode, exploding, kittens, fi...\n",
       "7585    [calgary, police, flood, road, closures, calgary]\n",
       "7586    [sismo, detectado, japìn, seismic, intensity, ...\n",
       "7587                                 [sirens, everywhere]\n",
       "7588    [breaking, isis, claims, responsibility, mosqu...\n",
       "7589                                    [omg, earthquake]\n",
       "7590    [severe, weather, bulletin, typhoon, ûïhannaph...\n",
       "7591    [heat, wave, warning, aa, ayyo, dei, plan, vis...\n",
       "7592    [group, suicide, bomber, detonated, explosives...\n",
       "7593    [heard, really, loud, bang, everyone, asleep, ...\n",
       "7594    [gas, thing, exploded, heard, screams, whole, ...\n",
       "7595    [nws, flash, flood, warning, continued, shelby...\n",
       "7596    [rt, livingsafely, nws, issues, severe, thunde...\n",
       "7597    [aircraft, debris, found, la, reunion, missing...\n",
       "7598    [fatherofthree, lost, control, car, overtaking...\n",
       "7599    [earthquake, ssw, anza, california, iphone, us...\n",
       "7600         [evacuation, order, lifted, town, roosevelt]\n",
       "7601    [breaking, la, refugio, oil, spill, may, costl...\n",
       "7602       [siren, went, wasnt, forney, tornado, warning]\n",
       "7603    [officials, say, quarantine, place, alabama, h...\n",
       "7604    [worldnews, fallen, powerlines, glink, tram, u...\n",
       "7605    [flip, side, im, walmart, bomb, everyone, evac...\n",
       "7606    [suicide, bomber, kills, saudi, security, site...\n",
       "7607    [stormchase, violent, record, breaking, el, re...\n",
       "7608    [two, giant, cranes, holding, bridge, collapse...\n",
       "7609    [ariaahrary, thetawniest, control, wild, fires...\n",
       "7610                                    [volcano, hawaii]\n",
       "7611    [police, investigating, ebike, collided, car, ...\n",
       "7612    [latest, homes, razed, northern, california, w...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df1['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devesh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10876, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.concat([tweet_df1,test_df1])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe for Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(df['text']):\n",
    "        words=[word.lower() for word in tweet if((word.isalpha()==1) )]\n",
    "        corpus.append(words)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10876/10876 [00:00<00:00, 189192.13it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus=create_corpus(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-7ad10bfa8427>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0membedding_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'"
     ]
    }
   ],
   "source": [
    "embedding_dict={}\n",
    "with open('../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=50\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index=tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words=len(word_index)+1\n",
    "\n",
    "embedding_matrix=np.zeros((num_words,200))\n",
    "\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    emb_vec=embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i]=emb_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiating TPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# detect and init the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\n",
    "# instantiate a distribution strategy\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tpu_strategy.scope():\n",
    "    model=Sequential()\n",
    "\n",
    "    embedding=Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "    model.add(embedding)\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    optimzer=Adam(learning_rate=1e-5)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tweet_pad[:tweet.shape[0]]\n",
    "test=tweet_pad[tweet.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split  training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\n",
    "print('Shape of train',X_train.shape)\n",
    "print(\"Shape of Validation \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(X_train,y_train,batch_size=16,epochs=15,validation_data=(X_test,y_test),verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a callback Function\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "callbacks = [\n",
    "             ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                               factor=0.2, \n",
    "                               patience=3, \n",
    "                               verbose=1)]\n",
    "\n",
    "filter_length1 = 3\n",
    "filter_length2 = 5\n",
    "dropout=0.5\n",
    "nb_filter = 64\n",
    "learning_rate=3e-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Cnn layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tpu_strategy.scope():\n",
    "    model=Sequential()\n",
    "\n",
    "    embedding=Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "    model.add(embedding)\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "   \n",
    "    model.add(Conv1D(64, 5,padding = 'same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate),metrics=['accuracy'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(X_train,y_train,batch_size=8,epochs=30,validation_data=(X_test,y_test),verbose=True,callbacks = callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "model1 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),input_length=MAX_LEN,trainable=False),\n",
    "    keras.layers.LSTM(100,return_sequences=True),\n",
    "    keras.layers.LSTM(200),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = model1.fit(X_train,y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history1.history['accuracy'], label='train')\n",
    "plt.plot(history1.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),input_length=MAX_LEN,trainable=False),\n",
    "    keras.layers.GRU(100,return_sequences=True),\n",
    "    keras.layers.GRU(200),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(X_train,y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),input_length=MAX_LEN,trainable=False),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(100,return_sequences=True)),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(200)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = model3.fit(X_train,y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre=model1.predict(test)\n",
    "y_pre=np.round(y_pre).astype(int).reshape(3263)\n",
    "sub1=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\n",
    "sub1.to_csv('submission1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre=model3.predict(test)\n",
    "y_pre=np.round(y_pre).astype(int).reshape(3263)\n",
    "sub3=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\n",
    "sub3.to_csv('submission3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
